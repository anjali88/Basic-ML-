{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pushpanjali_Banik_Scikit-learn.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjali88/Basic-ML-/blob/master/Pushpanjali_Banik_Scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qniekiBjsmH",
        "colab_type": "code",
        "outputId": "3823ab0b-d214-4372-ee80-4d7def621842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Excercise 1\n",
        "#Import statements\n",
        "#import scikit learn libraries when the are required to understand which specifc libraries are required\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#load classification dataset\n",
        "class_data = load_breast_cancer()\n",
        "class_X = pd.DataFrame(class_data.data, columns = class_data.feature_names)\n",
        "class_y = class_data.target\n",
        "\n",
        "#check the data has loaded successfully\n",
        "print(class_X.shape)\n",
        "print(class_y.shape)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 30)\n",
            "(569,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG16QyOBp_Jf",
        "colab_type": "code",
        "outputId": "b0d781ab-7ef4-42f0-d265-55c7aa1314f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#load the regression dataset\n",
        "boston = load_boston()\n",
        "boston_X = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
        "boston_y = boston.target\n",
        "\n",
        "##check the data has loaded successfully\n",
        "print(boston_X.shape)\n",
        "print(boston_y.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 13)\n",
            "(506,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf5AEMKXqh58",
        "colab_type": "code",
        "outputId": "bdab698c-3623-4a87-d1d0-4581e9972af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "#Excercise 2 - Rescaling and Standardizing Data\n",
        "# Rescaling using MinMaxScaler, which converts data to a specified range, usually (0,1) or (-1,1)\n",
        "# many machine learning algorithms assume data is on the same scale\n",
        "#import minmaxscaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#create feature - use one feature but may pass all the features to rescaler\n",
        "feature = class_X['texture error']\n",
        "#print feature prior to rescaling\n",
        "print(\"Features prior to rescaling:\\n\")\n",
        "print(feature[0:8])\n",
        "#print the shape\n",
        "print(\"\\nFeature shape prior to reshaping:\")\n",
        "print(feature.shape)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features prior to rescaling:\n",
            "\n",
            "0    0.9053\n",
            "1    0.7339\n",
            "2    0.7869\n",
            "3    1.1560\n",
            "4    0.7813\n",
            "5    0.8902\n",
            "6    0.7732\n",
            "7    1.3770\n",
            "Name: texture error, dtype: float64\n",
            "\n",
            "Feature shape prior to reshaping:\n",
            "(569,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLKZgv6SuZqE",
        "colab_type": "code",
        "outputId": "d867fcdb-c776-47a4-e774-efad74f2b4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# we used a pandas column, so data data has wrong shape for the function\n",
        "# we need a 2D tuple\n",
        "#reshape the data\n",
        "feature = feature.values.reshape(-1,1)\n",
        "\n",
        "# notice we now have a (samples,feature) tuple not just a (samples,) tuple\n",
        "print(\"\\nFeature shape after reshaping:\")\n",
        "feature.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Feature shape after reshaping:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNS67gfLvh3R",
        "colab_type": "code",
        "outputId": "100da5a7-6305-4f7e-f8fa-522d28d6faf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "#instantiate the scaler and pass in the range we want our value to be scaled within\n",
        "minmax_scale = MinMaxScaler(feature_range=(0,1))\n",
        "#Scale the feature - note the fit transform function, in the next example we will separate these steps\n",
        "scaled_feature = minmax_scale.fit_transform(feature)\n",
        "#view feautures we have rescaled\n",
        "print(\"Features after  rescaling:\\n\")\n",
        "scaled_feature[0:8]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features after  rescaling:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12046941],\n",
              "       [0.08258929],\n",
              "       [0.09430251],\n",
              "       [0.17587518],\n",
              "       [0.09306489],\n",
              "       [0.11713225],\n",
              "       [0.09127475],\n",
              "       [0.22471711]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAgBF9c-wPAT",
        "colab_type": "code",
        "outputId": "6b16c8c2-8cc6-4871-a16b-e3143f614d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "#standardising rescale features so that they are normally distributed. with a mean of 0 and std. deviation of 1\n",
        "#import appropriate library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#transform all features in dataset, look at it before transformation\n",
        "mean_std = pd.DataFrame(data={'mean':class_X.mean(), 'std':class_X.std()})\n",
        "\n",
        "#call dataframe\n",
        "mean_std[0:8]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean radius</th>\n",
              "      <td>14.127292</td>\n",
              "      <td>3.524049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean texture</th>\n",
              "      <td>19.289649</td>\n",
              "      <td>4.301036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean perimeter</th>\n",
              "      <td>91.969033</td>\n",
              "      <td>24.298981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean area</th>\n",
              "      <td>654.889104</td>\n",
              "      <td>351.914129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean smoothness</th>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.014064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean compactness</th>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.052813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concavity</th>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.079720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concave points</th>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.038803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           mean         std\n",
              "mean radius           14.127292    3.524049\n",
              "mean texture          19.289649    4.301036\n",
              "mean perimeter        91.969033   24.298981\n",
              "mean area            654.889104  351.914129\n",
              "mean smoothness        0.096360    0.014064\n",
              "mean compactness       0.104341    0.052813\n",
              "mean concavity         0.088799    0.079720\n",
              "mean concave points    0.048919    0.038803"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCCn6Rq_Y_zs",
        "colab_type": "code",
        "outputId": "34f4e912-7f8b-4815-c369-e89e65a4d139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#create scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#fit the scaler - this calculates the min and max values of the dates\n",
        "scaler.fit(class_X)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vn0EMhyZCWE",
        "colab_type": "code",
        "outputId": "e1b2341f-9859-4c87-e13b-7a06b030f7f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# transform the data using the fitted scaler - this applies the transform using the fit\n",
        "standardized = scaler.transform(class_X)\n",
        "# this shows the features transformed - note that this returns an array not a dataframe\n",
        "standardized[0:8]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.09706398e+00, -2.07333501e+00,  1.26993369e+00,\n",
              "         9.84374905e-01,  1.56846633e+00,  3.28351467e+00,\n",
              "         2.65287398e+00,  2.53247522e+00,  2.21751501e+00,\n",
              "         2.25574689e+00,  2.48973393e+00, -5.65265059e-01,\n",
              "         2.83303087e+00,  2.48757756e+00, -2.14001647e-01,\n",
              "         1.31686157e+00,  7.24026158e-01,  6.60819941e-01,\n",
              "         1.14875667e+00,  9.07083081e-01,  1.88668963e+00,\n",
              "        -1.35929347e+00,  2.30360062e+00,  2.00123749e+00,\n",
              "         1.30768627e+00,  2.61666502e+00,  2.10952635e+00,\n",
              "         2.29607613e+00,  2.75062224e+00,  1.93701461e+00],\n",
              "       [ 1.82982061e+00, -3.53632408e-01,  1.68595471e+00,\n",
              "         1.90870825e+00, -8.26962447e-01, -4.87071673e-01,\n",
              "        -2.38458552e-02,  5.48144156e-01,  1.39236330e-03,\n",
              "        -8.68652457e-01,  4.99254601e-01, -8.76243603e-01,\n",
              "         2.63326966e-01,  7.42401948e-01, -6.05350847e-01,\n",
              "        -6.92926270e-01, -4.40780058e-01,  2.60162067e-01,\n",
              "        -8.05450380e-01, -9.94437403e-02,  1.80592744e+00,\n",
              "        -3.69203222e-01,  1.53512599e+00,  1.89048899e+00,\n",
              "        -3.75611957e-01, -4.30444219e-01, -1.46748968e-01,\n",
              "         1.08708430e+00, -2.43889668e-01,  2.81189987e-01],\n",
              "       [ 1.57988811e+00,  4.56186952e-01,  1.56650313e+00,\n",
              "         1.55888363e+00,  9.42210440e-01,  1.05292554e+00,\n",
              "         1.36347845e+00,  2.03723076e+00,  9.39684817e-01,\n",
              "        -3.98007910e-01,  1.22867595e+00, -7.80083377e-01,\n",
              "         8.50928301e-01,  1.18133606e+00, -2.97005012e-01,\n",
              "         8.14973504e-01,  2.13076435e-01,  1.42482747e+00,\n",
              "         2.37035535e-01,  2.93559404e-01,  1.51187025e+00,\n",
              "        -2.39743838e-02,  1.34747521e+00,  1.45628455e+00,\n",
              "         5.27407405e-01,  1.08293217e+00,  8.54973944e-01,\n",
              "         1.95500035e+00,  1.15225500e+00,  2.01391209e-01],\n",
              "       [-7.68909287e-01,  2.53732112e-01, -5.92687167e-01,\n",
              "        -7.64463792e-01,  3.28355348e+00,  3.40290899e+00,\n",
              "         1.91589718e+00,  1.45170736e+00,  2.86738293e+00,\n",
              "         4.91091929e+00,  3.26373441e-01, -1.10409044e-01,\n",
              "         2.86593405e-01, -2.88378148e-01,  6.89701660e-01,\n",
              "         2.74428041e+00,  8.19518384e-01,  1.11500701e+00,\n",
              "         4.73268037e+00,  2.04751088e+00, -2.81464464e-01,\n",
              "         1.33984094e-01, -2.49939304e-01, -5.50021228e-01,\n",
              "         3.39427470e+00,  3.89339743e+00,  1.98958826e+00,\n",
              "         2.17578601e+00,  6.04604135e+00,  4.93501034e+00],\n",
              "       [ 1.75029663e+00, -1.15181643e+00,  1.77657315e+00,\n",
              "         1.82622928e+00,  2.80371830e-01,  5.39340452e-01,\n",
              "         1.37101143e+00,  1.42849277e+00, -9.56046689e-03,\n",
              "        -5.62449981e-01,  1.27054278e+00, -7.90243702e-01,\n",
              "         1.27318941e+00,  1.19035676e+00,  1.48306716e+00,\n",
              "        -4.85198799e-02,  8.28470780e-01,  1.14420474e+00,\n",
              "        -3.61092272e-01,  4.99328134e-01,  1.29857524e+00,\n",
              "        -1.46677038e+00,  1.33853946e+00,  1.22072425e+00,\n",
              "         2.20556166e-01, -3.13394511e-01,  6.13178758e-01,\n",
              "         7.29259257e-01, -8.68352984e-01, -3.97099619e-01],\n",
              "       [-4.76374665e-01, -8.35335303e-01, -3.87148067e-01,\n",
              "        -5.05650454e-01,  2.23742148e+00,  1.24433549e+00,\n",
              "         8.66301596e-01,  8.24655646e-01,  1.00540180e+00,\n",
              "         1.89000504e+00, -2.55070294e-01, -5.92661652e-01,\n",
              "        -3.21304185e-01, -2.89258217e-01,  1.56346702e-01,\n",
              "         4.45543649e-01,  1.60025198e-01, -6.91235537e-02,\n",
              "         1.34118807e-01,  4.86845840e-01, -1.65498247e-01,\n",
              "        -3.13836333e-01, -1.15009456e-01, -2.44320208e-01,\n",
              "         2.04851283e+00,  1.72161644e+00,  1.26324320e+00,\n",
              "         9.05887786e-01,  1.75406939e+00,  2.24180161e+00],\n",
              "       [ 1.17090767e+00,  1.60649427e-01,  1.13812505e+00,\n",
              "         1.09529491e+00, -1.23136226e-01,  8.82952423e-02,\n",
              "         3.00072399e-01,  6.46935108e-01, -6.43246179e-02,\n",
              "        -7.62332153e-01,  1.49883071e-01, -8.04939888e-01,\n",
              "         1.55410293e-01,  2.98627465e-01, -9.09029826e-01,\n",
              "        -6.51568010e-01, -3.10141387e-01, -2.28089026e-01,\n",
              "        -8.29666081e-01, -6.11217806e-01,  1.36898330e+00,\n",
              "         3.22882892e-01,  1.36832530e+00,  1.27521954e+00,\n",
              "         5.18640227e-01,  2.12149800e-02,  5.09552250e-01,\n",
              "         1.19671580e+00,  2.62475664e-01, -1.47304787e-02],\n",
              "       [-1.18516778e-01,  3.58450132e-01, -7.28668396e-02,\n",
              "        -2.18964911e-01,  1.60404905e+00,  1.14010235e+00,\n",
              "         6.10257495e-02,  2.81950258e-01,  1.40335463e+00,\n",
              "         1.66035318e+00,  6.43623001e-01,  2.90560957e-01,\n",
              "         4.90050986e-01,  2.33722421e-01,  5.88030871e-01,\n",
              "         2.68932704e-01, -2.32553954e-01,  4.35348506e-01,\n",
              "        -6.88004232e-01,  6.11668783e-01,  1.63762976e-01,\n",
              "         4.01047912e-01,  9.94485804e-02,  2.88594274e-02,\n",
              "         1.44796112e+00,  7.24785507e-01, -2.10538519e-02,\n",
              "         6.24195735e-01,  4.77640485e-01,  1.72643451e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9BZb-9tZI0b",
        "colab_type": "code",
        "outputId": "ff341eeb-fb69-4a5f-95b3-767be7c02554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "#have another look at the transformed data\n",
        "mean_std_transformed = pd.DataFrame(data={'mean':standardized.mean(), 'std':standardized.std()}, index=class_X.columns)\n",
        "\n",
        "# call the dataframe\n",
        "mean_std_transformed[0:8]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean radius</th>\n",
              "      <td>-6.118909e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean texture</th>\n",
              "      <td>-6.118909e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean perimeter</th>\n",
              "      <td>-6.118909e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean area</th>\n",
              "      <td>-6.118909e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean smoothness</th>\n",
              "      <td>-6.118909e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean compactness</th>\n",
              "      <td>-6.118909e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concavity</th>\n",
              "      <td>-6.118909e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concave points</th>\n",
              "      <td>-6.118909e-16</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             mean  std\n",
              "mean radius         -6.118909e-16  1.0\n",
              "mean texture        -6.118909e-16  1.0\n",
              "mean perimeter      -6.118909e-16  1.0\n",
              "mean area           -6.118909e-16  1.0\n",
              "mean smoothness     -6.118909e-16  1.0\n",
              "mean compactness    -6.118909e-16  1.0\n",
              "mean concavity      -6.118909e-16  1.0\n",
              "mean concave points -6.118909e-16  1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLrEiyw0ZMxG",
        "colab_type": "code",
        "outputId": "e7ed8569-45f6-4d08-9541-088b88b3aa14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Excercise 2 - Rescaling and Standardization\n",
        "load_boston()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
              " 'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
              "         4.9800e+00],\n",
              "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
              "         9.1400e+00],\n",
              "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
              "         4.0300e+00],\n",
              "        ...,\n",
              "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
              "         5.6400e+00],\n",
              "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
              "         6.4800e+00],\n",
              "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
              "         7.8800e+00]]),\n",
              " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
              "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
              " 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/boston_house_prices.csv',\n",
              " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
              "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
              "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
              "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
              "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
              "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
              "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
              "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
              "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
              "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
              "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
              "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
              "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
              "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
              "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
              "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
              "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
              "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
              "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
              "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
              "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
              "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
              "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
              "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
              "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
              "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
              "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
              "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
              "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
              "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
              "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
              "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
              "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
              "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
              "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
              "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
              "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
              "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
              "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
              "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
              "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
              "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
              "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
              "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
              "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
              "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEYTC20SZPEI",
        "colab_type": "code",
        "outputId": "dc7b9dc7-39ab-49f3-d3f6-c44925028c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "source": [
        "Class1 = load_boston()\n",
        "Class1_X = pd.DataFrame(Class1.data, columns = Class1.feature_names) #features\n",
        "Class1_y = Class1.target # labels\n",
        "print(Class1_X.shape)\n",
        "print(Class1_y.shape) #  check if data loaded successfully\n",
        "\n",
        "# FEATURE RESCALING\n",
        "from sklearn.preprocessing import MinMaxScaler # import required library\n",
        "#create feature - use one feature but may pass all the features to rescaler\n",
        "F1 = Class1_X['TAX']\n",
        "\n",
        "#print feature prior to rescaling\n",
        "print(\"Features prior to rescaling:\\n\")\n",
        "print(F1[0:10])\n",
        "\n",
        "#print the shape\n",
        "print(\"\\nFeature shape prior to reshaping:\")\n",
        "print(F1.shape)\n",
        "#Reshape the data into 2D tuple for the function\n",
        "F1 = F1.values.reshape(-1,1)\n",
        "print(\"\\nFeature shape after reshaping: \")\n",
        "print(F1.shape)\n",
        "F1[0:10]\n",
        "\n",
        "# instantiate the scaler and pass the feature range \n",
        "minmax_scale = MinMaxScaler(feature_range=(0, 1))\n",
        "#Scale the feature - note the fit transform function, in the next example we will separate these steps\n",
        "scaled_F1 = minmax_scale.fit_transform(F1)\n",
        "#view feautures we have rescaled\n",
        "print(\"Features after  rescaling:\\n\")\n",
        "scaled_F1[0:10]\n",
        "\n",
        "#STANDARDISING - using Standardscaler\n",
        "#standardise the features to be normally distributed. with a mean of 0 and std. deviation of 1\n",
        "from sklearn.preprocessing import StandardScaler # import appropriate library\n",
        "mean_std = pd.DataFrame(data={'mean':Class1_X.mean(), 'std':Class1_X.std()}) # transform all the features in the dataset\n",
        "mean_std[0:10] # call the dataframe\n",
        "print()\n",
        "scaler = StandardScaler() #create the standard scaler\n",
        "scaler.fit(Class1_X) #fit the scaler - this calculates the min and max values of the dates\n",
        "standardized1 = scaler.transform(Class1_X) # transform the data using the fitted scaler - this applies the transform using the fit\n",
        "standardized1[0:8]# this shows the features transformed - note that this returns an array not a dataframe\n",
        "mean_std_transformed = pd.DataFrame(data={'mean':standardized1.mean(), 'std':standardized1.std()}, index=Class1_X.columns) #have another look at the transformed data\n",
        "mean_std_transformed[0:10] # call the dataframe"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 13)\n",
            "(506,)\n",
            "Features prior to rescaling:\n",
            "\n",
            "0    296.0\n",
            "1    242.0\n",
            "2    242.0\n",
            "3    222.0\n",
            "4    222.0\n",
            "5    222.0\n",
            "6    311.0\n",
            "7    311.0\n",
            "8    311.0\n",
            "9    311.0\n",
            "Name: TAX, dtype: float64\n",
            "\n",
            "Feature shape prior to reshaping:\n",
            "(506,)\n",
            "\n",
            "Feature shape after reshaping: \n",
            "(506, 1)\n",
            "Features after  rescaling:\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>CRIM</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZN</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INDUS</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CHAS</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NOX</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RM</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DIS</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAD</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TAX</th>\n",
              "      <td>-1.114746e-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               mean  std\n",
              "CRIM  -1.114746e-15  1.0\n",
              "ZN    -1.114746e-15  1.0\n",
              "INDUS -1.114746e-15  1.0\n",
              "CHAS  -1.114746e-15  1.0\n",
              "NOX   -1.114746e-15  1.0\n",
              "RM    -1.114746e-15  1.0\n",
              "AGE   -1.114746e-15  1.0\n",
              "DIS   -1.114746e-15  1.0\n",
              "RAD   -1.114746e-15  1.0\n",
              "TAX   -1.114746e-15  1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVGfJXrgZZNM",
        "colab_type": "code",
        "outputId": "6979d0f6-36a5-44ce-aeda-027d21529c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "source": [
        "# NORMALIZER\n",
        "# Normalize samples individually to unit norm.\n",
        "# Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1 or l2) equals one.\n",
        "# Scaling inputs to unit norms is a common operation for text classification or clustering for instance\n",
        "from sklearn.preprocessing import Normalizer #import appropriate library\n",
        "transformer = Normalizer().fit(Class1_X) # fit does nothing\n",
        "print(transformer)\n",
        "normalised = transformer.transform(Class1_X)\n",
        "print(normalised)\n",
        "mean_std_transformed = pd.DataFrame(data={'mean':normalised.mean(), 'std':normalised.std()}, index=Class1_X.columns) #have another look at the transformed data\n",
        "mean_std_transformed[0:10] # call the dataframe"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalizer(copy=True, norm='l2')\n",
            "[[1.26388341e-05 3.59966795e-02 4.61957387e-03 ... 3.05971776e-02\n",
            "  7.93726783e-01 9.95908132e-03]\n",
            " [5.78529889e-05 0.00000000e+00 1.49769546e-02 ... 3.77071843e-02\n",
            "  8.40785474e-01 1.93620036e-02]\n",
            " [5.85729947e-05 0.00000000e+00 1.51744622e-02 ... 3.82044450e-02\n",
            "  8.43137761e-01 8.64965806e-03]\n",
            " ...\n",
            " [1.23765824e-04 0.00000000e+00 2.43009593e-02 ... 4.27762066e-02\n",
            "  8.08470305e-01 1.14884669e-02]\n",
            " [2.24644719e-04 0.00000000e+00 2.44548909e-02 ... 4.30471676e-02\n",
            "  8.06519433e-01 1.32831260e-02]\n",
            " [9.69214289e-05 0.00000000e+00 2.43887924e-02 ... 4.29308164e-02\n",
            "  8.11392431e-01 1.61092778e-02]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>CRIM</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZN</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INDUS</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CHAS</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NOX</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RM</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DIS</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAD</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TAX</th>\n",
              "      <td>0.123567</td>\n",
              "      <td>0.248303</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           mean       std\n",
              "CRIM   0.123567  0.248303\n",
              "ZN     0.123567  0.248303\n",
              "INDUS  0.123567  0.248303\n",
              "CHAS   0.123567  0.248303\n",
              "NOX    0.123567  0.248303\n",
              "RM     0.123567  0.248303\n",
              "AGE    0.123567  0.248303\n",
              "DIS    0.123567  0.248303\n",
              "RAD    0.123567  0.248303\n",
              "TAX    0.123567  0.248303"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsv_ION1Zav2",
        "colab_type": "code",
        "outputId": "3b589913-27d3-449b-ff49-5665d783a91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "#Model Evaluation and Metrics\n",
        "#testing and training splits\n",
        "from sklearn.model_selection import train_test_split\n",
        "# splits data into a test and training set\n",
        "# function shuffles our data before splitting (we will get both classes in both sets)\n",
        "# setting a random_state means that this shuffling will be consistent for each run\n",
        "#setting stratify=class_y tells the function to have an even number of class labels in each set\n",
        "X_train, X_test, y_train, y_test = train_test_split(class_X,class_y, test_size=0.3, random_state=1, stratify = class_y)\n",
        "\n",
        "# verify the stratifications using np.bincount\n",
        "print('Labels counts in y:', np.bincount(class_y))\n",
        "print('Percentage of class zeroes in class_y',np.round(np.bincount(class_y)[0]/len(class_y)*100))\n",
        "\n",
        "print(\"\\n\")\n",
        "print('Labels counts in y_train:', np.bincount(y_train))\n",
        "print('Percentage of class zeroes in y_train',np.round(np.bincount(y_train)[0]/len(y_train)*100))\n",
        "\n",
        "print(\"\\n\")\n",
        "print('Labels counts in y_test:', np.bincount(y_test))\n",
        "print('Percentage of class zeroes in y_test',np.round(np.bincount(y_test)[0]/len(y_test)*100))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels counts in y: [212 357]\n",
            "Percentage of class zeroes in class_y 37.0\n",
            "\n",
            "\n",
            "Labels counts in y_train: [148 250]\n",
            "Percentage of class zeroes in y_train 37.0\n",
            "\n",
            "\n",
            "Labels counts in y_test: [ 64 107]\n",
            "Percentage of class zeroes in y_test 37.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtPajTn7Zf2A",
        "colab_type": "text"
      },
      "source": [
        "Additional ways to divide data into test and training sets exist, and we cover K-fold cross validation in the Pipeline section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46H72GAoZiNl",
        "colab_type": "code",
        "outputId": "88ebddc1-6626-4dae-ac79-18ccae945f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#create a basiline model to benchmark our other estimators against\n",
        "#this can be simple estimator or we can use a dummy estimator to make predictions in a random manner\n",
        "#import appropriate statement\n",
        "from sklearn.dummy import DummyClassifier\n",
        "# creates our dummy classifier and the value we pass in to the strategy\n",
        "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
        "dummy.fit(X_train,y_train) # #train the model"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DummyClassifier(constant=None, random_state=1, strategy='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3tC7pJAZlEK",
        "colab_type": "code",
        "outputId": "defdb142-1242-4d6c-f754-a7d7d985c8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#evaluate model metrics\n",
        "dummy.score(X_test, y_test) # get an accuracy score"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47953216374269003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3yDxq4KZod3",
        "colab_type": "text"
      },
      "source": [
        "Remember that to get a score, we need to instantiate a model, fit it to the data, predict using unseen data, compare the predictions against actual data, and score the difference. This is true for classification and regression problems, and is true no matter the method used to get there.\n",
        "\n",
        "* So, in the end-to-end tutorials we split the training and test data, fitted our data to an estimator, and called the .predict method on the estimator to get our predictions, and then passed this to a scoring function (four steps)\n",
        "* In using the estimator.score() method above, we are passing in our split data and the method is then making predictions and returning the score (three steps).\n",
        "* In the cross_val_score() method used below we are effectively using one step as the method takes an estimator and our data and returns a score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLPyEdVlZuJl",
        "colab_type": "code",
        "outputId": "b7935840-b397-4802-ff55-720b3c2c5529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#hide all warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#fit a new estimator and use cross_val_score to get a score based defined metric\n",
        "#import statements\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic = LogisticRegression() # instantiate logistic regression classifier\n",
        "\n",
        "#pass estimator and data to the method. also specify the number of folds (default is 3)\n",
        "# the default scoring method is associated with the estimator we pass in\n",
        "# use scoring parameter to pass in different scoring methods. here we use recall\n",
        "cross_val_score(logistic, class_X, class_y, cv=5, scoring=\"recall\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.98611111, 0.97222222, 0.98591549, 0.95774648, 0.95774648])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtmHOziNZx5l",
        "colab_type": "code",
        "outputId": "ea988331-e0b7-492d-b9e4-d9698fd70954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "#Excercise 3\n",
        "\n",
        "#Part 1 -Implement binary classification scoring functions\n",
        "\n",
        "#Implement f1 score - also known as balanced F-score or F-measure\n",
        "# F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
        "# F1 = 2 * (precision * recall) / (precision + recall) \n",
        "#import appropriate statements\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1 = cross_val_score(logistic, class_X, class_y, cv=5, scoring='f1') \n",
        "print('f1')\n",
        "print(f1)\n",
        "f1macro = cross_val_score(logistic, class_X, class_y, cv=5, scoring='f1_macro') #Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "print('f1macro')\n",
        "print(f1macro)\n",
        "f1micro = cross_val_score(logistic, class_X, class_y, cv=5, scoring='f1_micro') #Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "print('f1micro')\n",
        "print(f1micro)\n",
        "f1weighted = cross_val_score(logistic, class_X, class_y, cv=5, scoring='f1_weighted') #Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
        "print('f1weighted')\n",
        "print(f1weighted)\n",
        "\n",
        "\n",
        "##implement precision score\n",
        "#The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
        "#The best value is 1 and the worst value is 0.\n",
        "#import appropriate statements\n",
        "from sklearn.metrics import precision_score\n",
        "precision = cross_val_score(logistic, class_X, class_y, cv=5, scoring='precision') \n",
        "print('precision')\n",
        "print(precision)\n",
        "precisionmacro = cross_val_score(logistic, class_X, class_y, cv=5, scoring='precision_macro') #Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
        "print('precisionmacro')\n",
        "print(precisionmacro)\n",
        "precisionmicro = cross_val_score(logistic, class_X, class_y, cv=5, scoring='precision_micro') #Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "print('precisionmicro')\n",
        "print(precisionmicro)\n",
        "precisionweighted = cross_val_score(logistic, class_X, class_y, cv=5, scoring='precision_weighted') #Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
        "print('precisionweighted')\n",
        "print(precisionweighted)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1\n",
            "[0.94666667 0.95238095 0.97902098 0.96453901 0.97142857]\n",
            "f1macro\n",
            "[0.92333333 0.9340218  0.9714382  0.95285774 0.96245847]\n",
            "f1micro\n",
            "[0.93043478 0.93913043 0.97345133 0.95575221 0.96460177]\n",
            "f1weighted\n",
            "[0.92921739 0.9386515  0.97338422 0.95585559 0.96476053]\n",
            "precision\n",
            "[0.91025641 0.93333333 0.97222222 0.97142857 0.98550725]\n",
            "precisionmacro\n",
            "[0.94161469 0.94166667 0.97391599 0.95083056 0.95866271]\n",
            "precisionmicro\n",
            "[0.93043478 0.93913043 0.97345133 0.95575221 0.96460177]\n",
            "precisionweighted\n",
            "[0.93370695 0.93956522 0.97348131 0.95611678 0.96555202]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9R4CrSAZ-Un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 2\n",
        "#Assessing our model's performance with the Receiving Operating Characteristic (ROC) Curve."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGGYWIA4aDTj",
        "colab_type": "text"
      },
      "source": [
        "A useful tool when predicting the probability of a binary outcome is the Receiver Operating Characteristic curve, or ROC curve. \n",
        "It is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate.\n",
        "\n",
        "The true positive rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive.\n",
        "\n",
        "\n",
        "* *True Positive Rate* = True Positives / (True Positives + False Negatives)\n",
        "\n",
        "The true positive rate is also referred to as sensitivity.\n",
        "\n",
        "* *Sensitivity* = True Positives / (True Positives + False Negatives)\n",
        "\n",
        "The false positive rate is calculated as the number of false positives divided by the sum of the number of false positives and the number of true negatives. It is also called the false alarm rate as it summarizes how often a positive class is predicted when the actual outcome is negative.\n",
        "\n",
        "* False Positive Rate = False Positives / (False Positives + True Negatives)\n",
        "\n",
        "The false positive rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.\n",
        "\n",
        "* Specificity = True Negatives / (True Negatives + False Positives)  \n",
        "Where:            False Positive Rate = 1 - Specificity\n",
        "      \n",
        "The ROC curve is a useful tool for a few reasons:\n",
        "* The curves of different models can be compared directly in general or for different thresholds.\n",
        "* The area under the curve (AUC) can be used as a summary of the model skill.\n",
        "The shape of the curve contains a lot of information, including what we might care about most for a problem, the expected false positive rate, and the false negative rate. To make this clear: a. Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives. b. Larger values on the y-axis of the plot indicate higher true positives and lower false negatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLKJG36JaKvj",
        "colab_type": "code",
        "outputId": "9dd7168c-76aa-4247-dd99-387fbd89ff24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# import roc_curve() to plot a ROC curve. \n",
        "# The function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. The function returns the false positive rates for each threshold, true positive rates for each threshold and thresholds.\n",
        "from sklearn.metrics import roc_curve \n",
        "# import roc_auc_score() to calculate AUC for ROC\n",
        "# AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively.\n",
        "from sklearn.metrics import roc_auc_score \n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "#shuffle and split training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(class_X,class_y, test_size=0.5, random_state=1)\n",
        "logistic = LogisticRegression()\n",
        "#predict\n",
        "Predict_Y = svm.SVC(kernel='linear', probability=True, random_state=1)\n",
        "print(Predict_Y)\n",
        "score_Y = Predict_Y.fit(X_train, y_train).decision_function(X_test)\n",
        "print(score_Y)\n",
        "#compute ROC curve and ROC area\n",
        "fpr, tpr, threshold = roc_curve(y_test, score_Y)\n",
        "roc_auc = roc_auc_score(y_test, score_Y)\n",
        "print(roc_auc)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='red', label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
            "    kernel='linear', max_iter=-1, probability=True, random_state=1,\n",
            "    shrinking=True, tol=0.001, verbose=False)\n",
            "[-1.06923266e+00 -1.29891176e+00  5.61895210e+00 -3.86980040e+00\n",
            " -1.88558814e-01 -6.25936209e+00 -3.95054394e+00  9.59540708e-02\n",
            "  4.01447965e+00  3.82038204e+00  3.63341162e+00 -4.38891749e+00\n",
            " -2.94887382e+00  4.40562556e+00  8.43109984e-01  3.42275087e-01\n",
            "  2.99971461e+00  3.36922732e+00  6.23624054e+00 -7.38527470e+00\n",
            "  4.40167439e+00  2.50609190e+00 -1.27530280e+00  2.87881002e+00\n",
            " -3.03204447e+00  1.10942621e+00  2.46355750e+00 -1.04239785e+01\n",
            " -1.81838538e+01 -2.87980800e+00 -8.53657025e+00  5.51272883e+00\n",
            " -1.00605766e+01 -4.06640033e+00  4.91427365e+00  3.74079660e+00\n",
            " -4.43476802e+00  2.75477264e+00 -5.31823157e-01  4.99311746e+00\n",
            "  6.74786021e+00  9.72271499e-01  1.96420850e+00  4.41307146e+00\n",
            "  2.78465875e+00 -2.67312982e+00  7.73322809e+00  6.90793245e+00\n",
            "  1.07239974e+00 -2.92079812e+00 -6.99689199e+00 -2.57654301e+01\n",
            "  4.25604618e+00  5.62641845e+00  5.25682842e+00  3.05419720e+00\n",
            "  4.49722566e+00  2.66771760e+00  4.10701024e+00  7.05267416e+00\n",
            "  5.68908592e+00 -4.12677026e+00  1.03895883e+00 -1.35746085e+00\n",
            "  4.12233903e+00  1.78406453e+00  2.58788561e+00 -9.86991084e+00\n",
            "  5.38697239e+00  3.28791927e+00  6.51335400e+00  6.74849130e+00\n",
            " -1.54475902e-01 -1.77436752e+00  7.48972501e+00 -1.40647361e+01\n",
            "  6.24672151e+00  1.55004106e+00  5.22138582e+00 -7.39287946e+00\n",
            "  6.30016907e+00 -1.09764454e+01  4.33606881e+00 -5.02931553e+00\n",
            "  4.56190591e+00  4.83693528e+00 -1.35542560e+00  2.33678744e+00\n",
            " -5.48357681e+00  5.47388272e+00  3.40526898e+00 -1.03121228e+01\n",
            "  5.95128556e+00  7.18672376e+00 -7.97427638e+00 -2.57491842e+00\n",
            "  9.14538809e+00  2.52833036e+00  4.73801796e+00  3.45219010e+00\n",
            "  2.42306269e+00  4.56268450e+00  6.14186344e+00  1.73313062e-01\n",
            "  3.93384171e+00  3.99547721e+00  7.19210382e+00  5.60146407e+00\n",
            " -1.86280183e+00 -4.45576243e+00 -2.09171199e+00  5.75434802e+00\n",
            " -2.95365951e-01  4.70478233e+00 -4.05557574e+00 -2.90797554e+00\n",
            "  8.41072479e+00  4.58627920e+00  6.73528175e+00  4.47720858e+00\n",
            "  2.44092216e+00 -7.95565565e+00 -7.64718185e+00  4.76480302e+00\n",
            "  4.22422243e+00 -2.42023880e+01 -8.01064897e+00 -5.77216758e-01\n",
            " -1.90993273e+01 -1.28793867e+01  1.46444231e+00  3.07273810e+00\n",
            "  4.17991269e+00 -1.81092685e+01  8.28886115e+00 -5.16337813e+00\n",
            " -4.81726298e+00  1.26144960e+00  4.53923995e+00 -5.26757108e-01\n",
            " -2.01455862e+00 -4.15763251e+00  4.23692986e+00 -7.27000054e+00\n",
            "  1.20941060e+00 -7.36274932e+00  7.07881600e+00  3.97165457e+00\n",
            "  5.51941867e+00 -1.33853007e+01  3.04423107e+00  5.45089014e+00\n",
            " -4.48242119e-02 -2.67924705e-02  2.54806517e+00  5.09091570e+00\n",
            "  3.41876674e+00  2.74458881e-01  3.68037032e+00  5.80096550e+00\n",
            "  5.24908423e+00  4.04189594e+00 -1.93758273e-01  1.83950401e+00\n",
            "  2.85230793e+00  2.71769609e+00 -1.02730986e+01 -1.03961829e+01\n",
            " -9.89861043e+00  4.11007176e+00  4.45065770e+00  5.11743152e+00\n",
            "  5.17006143e+00 -1.65045954e+00  5.39866266e+00  5.89700013e+00\n",
            "  7.28103468e+00  4.66930038e+00  9.58434282e-01  8.90812382e+00\n",
            "  5.65569765e+00  6.06815990e+00  1.82109517e+00  5.21880482e+00\n",
            "  3.79996207e+00 -2.32241103e+01 -1.45721205e+01  3.44221236e+00\n",
            " -7.73112215e+00  7.02749542e+00  2.36936516e+00  1.70374965e+00\n",
            " -3.43470593e+00 -2.72168930e+00 -1.05081026e+01  1.38895230e+00\n",
            "  8.80466751e+00 -4.65604968e+00 -5.57551984e+00  1.78418919e+00\n",
            " -1.06914367e-01  2.27187818e+00  2.04458697e+00  1.07558254e-01\n",
            " -1.54586571e+01  2.58676464e+00  3.62464430e+00  2.43380563e+00\n",
            " -2.13889559e+00 -2.50605178e+00  1.09840979e+00  3.85331897e+00\n",
            "  7.05800970e+00  1.85941313e+00 -1.01814676e+01  9.11251957e-01\n",
            "  4.65169503e+00  1.80838832e+00  4.61956809e+00  5.11746577e+00\n",
            "  2.48403637e+00  5.11126059e+00 -8.32973600e+00  5.81089189e+00\n",
            " -9.29515935e+00  8.60426148e+00  6.44793426e+00 -2.24326390e+01\n",
            " -1.13191306e+01  2.69529426e+00 -3.97752631e+00  6.54102584e+00\n",
            "  1.52111604e+00 -2.85652670e+01 -1.17347812e+00 -4.44171710e-01\n",
            " -9.15052586e+00 -1.39889931e+01  1.62171609e+00 -4.25177413e-01\n",
            "  5.44905888e+00  1.68648565e+00  4.58436715e+00  3.06271192e+00\n",
            "  5.32663388e+00 -1.55561833e+00  4.42694420e+00  1.71404384e+00\n",
            " -1.42808090e+01 -6.46803154e+00 -1.76748754e+01 -7.66904837e+00\n",
            "  3.20228642e+00 -4.65925128e+00  1.27586543e+00  4.32544738e+00\n",
            " -9.87332832e-01  1.85404044e+00 -6.86444731e+00  4.44821450e+00\n",
            "  4.62075959e+00  7.89362141e+00  3.48416434e+00  2.84489124e-01\n",
            "  2.55008441e+00  4.42101218e+00 -7.77730981e+00  4.05484193e+00\n",
            "  6.77490454e+00  5.44765724e+00 -7.92636997e-01 -2.08818848e+01\n",
            "  1.59055798e+00  6.42743969e+00 -1.68271730e+01 -1.93352755e+00\n",
            " -1.95925010e-01  3.11621699e+00  1.40854772e+00 -5.01617613e+00\n",
            "  6.58683529e+00  5.90889244e+00  4.83353088e+00  6.62014860e+00\n",
            "  3.13678217e+00]\n",
            "0.9666595540381948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB32+eJySYYp4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgJTIpwS3evD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What are the main ways of evaluating a mutliclass classification problem?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV7vP2-X3Y5r",
        "colab_type": "text"
      },
      "source": [
        "Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.\n",
        "\n",
        "In multiclass classification task, the notions of precision, recall, and F-measures can be applied to each label independently. \n",
        "* Note that if all labels are included, “micro”-averaging in a multiclass setting will produce precision, recall and  that are all identical to accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7YuQ4393d_4",
        "colab_type": "code",
        "outputId": "dcb55b2b-180a-4faf-9e09-2f57b7b025d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "#Part 4\n",
        "## Implement a baseline model and evaluation metric for the Boston dataset (regression)\n",
        "load_boston()\n",
        "from sklearn.model_selection import train_test_split\n",
        "boston_X_train,boston_X_test, boston_y_train, boston_y_test = train_test_split(Class1_X,Class1_y, test_size=0.5, random_state=1)\n",
        "\n",
        "from sklearn.dummy import DummyRegressor\n",
        "# creates our dummy regressor and the value we pass in to the strategy\n",
        "dummy_regr = DummyRegressor(strategy='mean')\n",
        "boston1_Y = dummy_regr.fit(boston_X_train,boston_y_train) # train the model\n",
        "print(boston1_Y)\n",
        "X1_predict = dummy_regr.predict(boston_X_train) #predict\n",
        "print(\"\\n\")\n",
        "print(X1_predict)\n",
        "boston_score = dummy_regr.score(boston_X_test,boston_y_test)\n",
        "print(\"\\n\")\n",
        "print(boston_score)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DummyRegressor(constant=None, quantile=None, strategy='mean')\n",
            "\n",
            "\n",
            "[22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032 22.04664032\n",
            " 22.04664032]\n",
            "\n",
            "\n",
            "-0.010773786729562529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD62bFHw3hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Excercise 4: Classification and regression machine learning algorithms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY7Xxft4xPeA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "> **DECISION TREE ALGORITHM (DT)**\n",
        "\n",
        "\n",
        "Decision tree classifiers are attractive models when focusing on interpretability. The model breaks down data by making decision based on asking a series of question. Based on features in our training dataset, the decision tree model learns a series of questions to infer the class labels of the dataset.\n",
        "\n",
        "* Using decision tree algorithm we start at the tree root and split the data on the feature that results in the largest information gain (IG) \n",
        "\n",
        " IG is simply the difference between impurity of the parent node and the sum of child node impurities. The lower the child node impurities, the larger the IG. \n",
        "\n",
        " However, to reduce combinational search space, most libraries implement binary decision trees, where each parent node is split into two child nodes (D-left and D-right)\n",
        "\n",
        "* In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure - means the training examples at each node all belong to the same class. \n",
        "* to avoid overfitting since the above step can result in a very deep tree with many modes, we typically prune the tree by setting a limit for max. depth of the tree\n",
        "\n",
        "\n",
        "\n",
        "> **SUPPORT VECTOR MACHINE ALGORITHM (SVM)**\n",
        "\n",
        "Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
        "\n",
        "The main objective is to segregate the given dataset in the best possible way. The distance between the either nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum marginal hyperplane in the following steps:\n",
        "\n",
        "* Generate hyperplanes which segregates the classes in the best way. Left-hand side figure showing three hyperplanes black, blue and orange. Here, the blue and orange have higher classification error, but the black is separating the two classes correctly.\n",
        "\n",
        "* Select the right hyperplane with the maximum segregation from the either nearest data points as shown in the right-hand side figure.\n",
        "\n",
        "The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space. In other words, you can say that it converts nonseparable problem to separable problems by adding more dimension to it. It is most useful in non-linear separation problem. Kernel trick helps you to build a more accurate classifier.\n",
        "\n",
        "* Linear Kernel A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n",
        "                   K(x, xi) = sum(x * xi)\n",
        "Where d is the degree of the polynomial. d=1 is similar to the linear transformation. The degree needs to be manually specified in the learning algorithm.\n",
        "*Polynomial Kernel A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.\n",
        "                   K(x,xi) = 1 + sum(x * xi)^d\n",
        "*Radial Basis Function Kernel The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in infinite dimensional space.\n",
        "                   K(x,xi) = exp(-gamma * sum((x – xi^2))\n",
        "gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm.\n",
        "\n",
        "> **LOGISTRIC REGRESSION ALGORITHM(LR)**\n",
        "\n",
        "The logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.Logistic Regression is used when the dependent variable(target) is categorical.\n",
        "For example,\n",
        "* To predict whether an email is spam (1) or (0)\n",
        "* Whether the tumor is malignant (1) or not (0)\n",
        "\n",
        "> **K NEAREST NEIGHBOR ALGORITHM(KNN)**\n",
        "\n",
        "KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier.\n",
        "\n",
        "In KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case. Suppose P1 is the point, for which label needs to predict. First, you find the one closest point to P1 and then the label of the nearest point assigned to P1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8u3Oenp1wJ_",
        "colab_type": "code",
        "outputId": "65268ad2-d87b-415a-c4d7-c44f682fcb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# using train_test_split function, we randomly split the boston_X and boston_y arrays into 50% test data and 50% training data\n",
        "# using train_test_split function, we shuffle the training dataset internally before splitting\n",
        "# via random_state, we provided a fixed random seed (random_state=1) for internal pseudo-random generator used for shuffling dataset prior to splitting\n",
        "X_train,X_test,y_train,y_test = train_test_split(class_X, class_y, test_size=0.5, random_state=1)\n",
        "\n",
        "# DECISION TREE (DT)\n",
        "\n",
        "#import approriate library\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#train a decision tree of max. depth of 4, using Gini impurity as criterion for impurity\n",
        "DTclassifier = DecisionTreeClassifier(criterion = 'gini', max_depth=4, random_state=1) \n",
        "DTclassifier.fit(X_train, y_train)\n",
        "Y_pred = DTclassifier.predict(X_test)\n",
        "# To check the accuracy we need to import confusion_matrix method of metrics class. \n",
        "# The confusion matrix is a way of tabulating the number of mis-classifications, i.e., the number of predicted classes which ended up in a wrong classification bin based on the true classes.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, Y_pred)\n",
        "print('Decision Tree Confusion Matrix:\\n', cm)\n",
        "#Return the mean accuracy on the given test data and labels.\n",
        "DTscore = DTclassifier.score(X_test, y_test)\n",
        "print('Decision Tree Score:\\n', DTscore)\n",
        "print(\"\\n\")\n",
        "\n",
        "# SUPPORT VECTOR MACHINE(SVM)\n",
        "\n",
        "#Using SVC method of svm class to use Kernel SVM Algorithm\n",
        "from sklearn.svm import SVC\n",
        "kernel_SVM_classifier = SVC(kernel = 'rbf', random_state = 0)\n",
        "kernel_SVM_classifier.fit(X_train, y_train)\n",
        "Y1_pred = kernel_SVM_classifier.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, Y1_pred)\n",
        "print('Kernel SVM Confusion Matrix:\\n', cm)\n",
        "#Return the mean accuracy on the given test data and labels.\n",
        "kernalSVMscore = kernel_SVM_classifier.score(X_test, y_test)\n",
        "print('kernal SVM Score:\\n', kernalSVMscore)\n",
        "print(\"\\n\")\n",
        "\n",
        "##Using SVC method of svm class to use Support Vector Machine Algorithm\n",
        "from sklearn.svm import SVC\n",
        "SVC_classifier = SVC(kernel = 'linear', random_state = 0)\n",
        "SVC_classifier.fit(X_train, y_train)\n",
        "Y2_pred = SVC_classifier.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, Y2_pred)\n",
        "print('SVC Confusion Matrix:\\n', cm)\n",
        "#Return the mean accuracy on the given test data and labels.\n",
        "SVCscore = SVC_classifier.score(X_test, y_test)\n",
        "print('SVC Score:\\n', SVCscore)\n",
        "print(\"\\n\")\n",
        "\n",
        "#LOGISTIC REGRESSION (LR)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LR_classifier = LogisticRegression(random_state = 0)\n",
        "LR_classifier.fit(X_train, y_train)\n",
        "Y3_pred = LR_classifier.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, Y3_pred)\n",
        "print('LR Confusion Matrix:\\n', cm)\n",
        "#Return the mean accuracy on the given test data and labels.\n",
        "LRscore = LR_classifier.score(X_test, y_test)\n",
        "print('LR Score:\\n', LRscore)\n",
        "print(\"\\n\")\n",
        "\n",
        "#K NEAREST NEIGHBOR (KNN)\n",
        "\n",
        "# KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "KNN_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "KNN_classifier.fit(X_train, y_train)\n",
        "Y4_pred = KNN_classifier.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, Y4_pred)\n",
        "print('KNN Confusion Matrix:\\n', cm)\n",
        "#Return the mean accuracy on the given test data and labels.\n",
        "KNNscore = KNN_classifier.score(X_test, y_test)\n",
        "print('KNN Score:\\n', KNNscore)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree Confusion Matrix:\n",
            " [[ 93  10]\n",
            " [ 23 159]]\n",
            "Decision Tree Score:\n",
            " 0.8842105263157894\n",
            "\n",
            "\n",
            "Kernel SVM Confusion Matrix:\n",
            " [[  0 103]\n",
            " [  0 182]]\n",
            "kernal SVM Score:\n",
            " 0.6385964912280702\n",
            "\n",
            "\n",
            "SVC Confusion Matrix:\n",
            " [[ 94   9]\n",
            " [ 11 171]]\n",
            "SVC Score:\n",
            " 0.9298245614035088\n",
            "\n",
            "\n",
            "LR Confusion Matrix:\n",
            " [[ 90  13]\n",
            " [  8 174]]\n",
            "LR Score:\n",
            " 0.9263157894736842\n",
            "\n",
            "\n",
            "KNN Confusion Matrix:\n",
            " [[ 86  17]\n",
            " [ 10 172]]\n",
            "KNN Score:\n",
            " 0.9052631578947369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9ZvhJA85ZuL",
        "colab_type": "code",
        "outputId": "23e01102-0320-4bb5-c05d-d9efece8fac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "source": [
        "#Decision tree algorithm and plotting decision surface of decision tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "iris_data = load_iris()\n",
        "# features\n",
        "iris_X = pd.DataFrame(iris_data.data, columns = iris_data.feature_names)\n",
        "# labels\n",
        "iris_y = iris_data.target\n",
        "# check the data has loaded successfully \n",
        "print(iris_X.shape)\n",
        "print(iris_y.shape)\n",
        "print(iris_data.feature_names)\n",
        "print('classlabels:', np.unique(iris_y))\n",
        "from sklearn.model_selection import train_test_split\n",
        "#using train_test_split function, we randomly split the boston_X and boston_y arrays into 50% test data and 50% training data\n",
        "# using train_test_split function, we shuffle the training dataset internally before splitting\n",
        "# via random_state, we provided a fixed random seed (random_state=1) for internal pseudo-random generator used for shuffling dataset prior to splitting\n",
        "iris_X_train,iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X,iris_y, test_size=0.5, random_state=1)\n",
        "#import approriate library\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#train a decision tree of max. depth of 4, using Gini impurity as criterion for impurity\n",
        "tree_model = DecisionTreeClassifier(criterion = 'gini', max_depth=4, random_state=1) \n",
        "treefit = tree_model.fit(iris_X_train, iris_y_train)\n",
        "X_combined = np.vstack((iris_X_train, iris_X_test))\n",
        "y_combined = np.hstack((iris_y_train, iris_y_test))\n",
        "\n",
        "#decision surface of a decision tree\n",
        "from sklearn.tree import plot_tree\n",
        "# Parameters\n",
        "n_classes = 3\n",
        "plot_colors = \"ryb\"\n",
        "plot_step = 0.02\n",
        "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
        "                                [1, 2], [1, 3], [2, 3]]):\n",
        "    # We only take the two corresponding features\n",
        "    iris_X = iris_data.data[:, pair]\n",
        "    iris_y = iris_data.target\n",
        "    # Train\n",
        "    clf = DecisionTreeClassifier().fit(iris_X, iris_y)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.subplot(2, 3, pairidx + 1)\n",
        "\n",
        "    x_min, x_max = iris_X[:, 0].min() - 1, iris_X[:, 0].max() + 1\n",
        "    y_min, y_max = iris_X[:, 1].min() - 1, iris_X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                         np.arange(y_min, y_max, plot_step))\n",
        "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "    plt.xlabel(iris_data.feature_names[pair[0]])\n",
        "    plt.ylabel(iris_data.feature_names[pair[1]])\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(n_classes), plot_colors):\n",
        "        idx = np.where(iris_y == i)\n",
        "        plt.scatter(iris_X[idx, 0], iris_X[idx, 1], c=color, label=iris_data.target_names[i],\n",
        "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
        "\n",
        "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
        "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
        "plt.axis(\"tight\")\n",
        "\n",
        "plt.figure()\n",
        "clf = DecisionTreeClassifier().fit(iris_data.data, iris_data.target)\n",
        "plot_tree(clf, filled=True)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 4)\n",
            "(150,)\n",
            "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "classlabels: [0 1 2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6smKPvKwjjT",
        "colab_type": "code",
        "outputId": "e5335afb-4b61-4c95-829e-fb62372042f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Excercise 5 - Regularization\n",
        "\n",
        "# For regularization in regression we instantiate new models\n",
        "# Lasso regression is used for L1 Regularization \n",
        "#import appropriate statement\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "#create lasso regression with alpha value - this is our hyperparameter\n",
        "regression = Lasso(alpha=0.5)\n",
        "#fit the linear regression\n",
        "model_L1 = regression.fit(class_X, class_y)\n",
        "#Ride regression is L2 Regularization\n",
        "#import statement\n",
        "from sklearn.linear_model import Ridge\n",
        "#create ridge regression with an alpha value\n",
        "regression = Ridge(alpha=0.5)\n",
        "#Fit the linear regression\n",
        "model_l2 = regression.fit(class_X, class_y)\n",
        "\n",
        "# Part 1\n",
        "# Get a score for both the models. \n",
        "# Compare the results and see if one type of regularization works better than another.\n",
        "Lasso_score = model_L1.score(X_test, y_test)\n",
        "print('Lasso Score:\\n', Lasso_score)\n",
        "Ridge_score = model_l2.score(X_test, y_test)\n",
        "print('Ridge Score:\\n', Ridge_score)\n",
        "\n",
        "# Part 2\n",
        "# Implement ElasticNet estimator\n",
        "from sklearn.linear_model import ElasticNet\n",
        "enet = ElasticNet(alpha=0.5, l1_ratio=0.7)\n",
        "model_l3 = enet.fit(class_X, class_y)\n",
        "enet_score = model_l3.score(X_test, y_test)\n",
        "print('enet Score:\\n', enet_score)\n",
        "\n",
        "# Part3\n",
        "# RidgeCV and LassoCV to explore the best regularization parameters for each estimator\n",
        "from sklearn.linear_model import RidgeCV\n",
        "ridgecv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
        "model_l4 = ridgecv.fit(class_X, class_y)\n",
        "ridgecv_score = model_l4.score(class_X, class_y)\n",
        "print('RidgeCV Score:\\n', ridgecv_score)\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "lassocv = LassoCV(cv=5, random_state=0)\n",
        "model_l5 = lassocv.fit(class_X, class_y)\n",
        "lassocv_score = model_l5.score(class_X, class_y)\n",
        "print('LassoCV Score:\\n', lassocv_score)\n",
        "\n",
        "# Part 4\n",
        "# Which parameters can be set to help prevent overfitting in DecisionTreeClassifier\n",
        "# To avoid overfitting due to very deep tree with many modes, we can typically prune setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree. "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lasso Score:\n",
            " 0.5824055808265075\n",
            "Ridge Score:\n",
            " 0.7114428906761474\n",
            "enet Score:\n",
            " 0.6027404700941736\n",
            "RidgeCV Score:\n",
            " 0.7699548108739226\n",
            "LassoCV Score:\n",
            " 0.6617948384330969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRq6dVKlgKlp",
        "colab_type": "code",
        "outputId": "75f0f3ab-1087-47db-d047-bc381b902360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        }
      },
      "source": [
        "# Excercise 6 - Optimizing parameters\n",
        "# Implementation of RandomizedSearchCV - use this method to search over defined hyperparameters, like GridSearchCV, however a fixed number of parameters are sampled, as defined by n_iter parameter.\n",
        "# import libraries - note we use scipy for generating a uniform distribution\n",
        "# hide all warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from scipy.stats import uniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "#create logistic regression\n",
        "logistic = LogisticRegression()\n",
        "from pprint import pprint\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(logistic.get_params())\n",
        "# creare hyperparameters as a list, as in type regularization penalty\n",
        "penalty = ['l1', 'l2']\n",
        "# or as a distributor of values to sample from -'C' is the hyperparameter controlling the size of the regularisation penalty\n",
        "C = uniform(loc=0, scale=4)\n",
        "# we need to pass these parameters as a dictornary of {param_name: values}\n",
        "hyperparameters = dict(C=C, penalty=penalty)\n",
        "print(hyperparameters)\n",
        "#instantiate our model\n",
        "randomizedsearch = RandomizedSearchCV(logistic, hyperparameters, random_state=1, n_iter=200, cv=5, verbose=0, n_jobs=-1)\n",
        "#fit the model\n",
        "best_model = randomizedsearch.fit(class_X, class_y)\n",
        "print(best_model)\n",
        "\n",
        "# Part 1\n",
        "# find out what the best parameters were\n",
        "# predict using the best estimator returned by the search\n",
        "\n",
        "# best_params = Parameter setting that gave the best results on the hold out data.\n",
        "bestparam = best_model.best_params_\n",
        "print('Best Parameters:\\n', bestparam)\n",
        "# best_estimator = Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data.\n",
        "bestestimator = best_model.best_estimator_\n",
        "print('best estimator:\\n', bestestimator)\n",
        "# best_score = Mean cross-validated score of the best_estimator.\n",
        "bestestimator_score = bestestimator.score(class_X, class_y)\n",
        "print('Best Estimator Score:\\n', bestestimator_score)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters currently in use:\n",
            "\n",
            "{'C': 1.0,\n",
            " 'class_weight': None,\n",
            " 'dual': False,\n",
            " 'fit_intercept': True,\n",
            " 'intercept_scaling': 1,\n",
            " 'l1_ratio': None,\n",
            " 'max_iter': 100,\n",
            " 'multi_class': 'warn',\n",
            " 'n_jobs': None,\n",
            " 'penalty': 'l2',\n",
            " 'random_state': None,\n",
            " 'solver': 'warn',\n",
            " 'tol': 0.0001,\n",
            " 'verbose': 0,\n",
            " 'warm_start': False}\n",
            "{'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa077ec8e10>, 'penalty': ['l1', 'l2']}\n",
            "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
            "                   estimator=LogisticRegression(C=1.0, class_weight=None,\n",
            "                                                dual=False, fit_intercept=True,\n",
            "                                                intercept_scaling=1,\n",
            "                                                l1_ratio=None, max_iter=100,\n",
            "                                                multi_class='warn', n_jobs=None,\n",
            "                                                penalty='l2', random_state=None,\n",
            "                                                solver='warn', tol=0.0001,\n",
            "                                                verbose=0, warm_start=False),\n",
            "                   iid='warn', n_iter=200, n_jobs=-1,\n",
            "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa077ec8e10>,\n",
            "                                        'penalty': ['l1', 'l2']},\n",
            "                   pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
            "                   return_train_score=False, scoring=None, verbose=0)\n",
            "Best Parameters:\n",
            " {'C': 1.828819231947953, 'penalty': 'l2'}\n",
            "best estimator:\n",
            " LogisticRegression(C=1.828819231947953, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Best Estimator Score:\n",
            " 0.961335676625659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txrOc_sIUAki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 2: Implement RandomSearchCV on the Boston dataset\n",
        "# hide all warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.linear_model import Lasso\n",
        "from scipy.stats import uniform # used to generate parameters\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# instantiate model\n",
        "LASSO = Lasso()\n",
        "from pprint import pprint\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(LASSO.get_params())\n",
        "# generate hyper-parameters\n",
        "alpha = [0.1,0.2,0.3,0.4,0.5,1]\n",
        "fit_intercept = [True, False]\n",
        "# create  a dictionary of hyperparameters\n",
        "hyperparameters = dict(alpha=alpha, fit_intercept=fit_intercept)\n",
        "# instantiate random search model\n",
        "randomizedsearch = RandomizedSearchCV(LASSO, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,n_jobs=-1)\n",
        "# fit the model to the data\n",
        "best_model_boston = randomizedsearch.fit(boston_X, boston_y)\n",
        "bestparam_boston = best_model_boston.best_params_\n",
        "print('Boston Best Parameters:\\n', bestparam_boston)\n",
        "# best_estimator = Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data.\n",
        "bestestimator_boston = best_model_boston.best_estimator_\n",
        "print('Boston best estimator:\\n', bestestimator_boston)\n",
        "# best_score = Mean cross-validated score of the best_estimator.\n",
        "bestestimator_score = bestestimator_boston.score(boston_X, boston_y)\n",
        "print('Boston Best Estimator Score:\\n', bestestimator_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J23ludoFYP8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Excercise 7- Ensemble models\n",
        "# They combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone\n",
        "# Several different approaches to achieve this, including majority voting ensemble methods, which we select the class label that has been predicted by the majority of classifiers.\n",
        "# The ensemble can be built from different classification algorithms, such as decision trees, support vector machines, logistic regression classifiers, and so on. Alternatively, we can also use the same base classification algorithm, fitting different subsets of the training set.\n",
        "# Indeed, Majority voting will work best if the classifiers used are different from each other and/or trained on different datasets (or subsets of the same data) in order for their errors to be uncorrelated.\n",
        "\n",
        "#import classifiers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# create models\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "# An ensemble of them\n",
        "# here we select hard voting, which returns the majority of the predictions, not an average of probabilities\n",
        "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svm', svm_clf)], voting='hard')\n",
        "\n",
        "# we can cycle throgh individual estimators\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "\n",
        "#fit the training set\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "#predict\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_QIdPxZeKGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "61cc0508-5ca0-4a06-b1f6-e743a7a18e74"
      },
      "source": [
        "# Part 1 -Implement Soft voting\n",
        "# If all the classifiers used in the ensemble are able to estimate class probabilities (they have a predict_proba() method that can be called on the estimator), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. It often achieves higher performance than hard voting because it gives more weight to highly confident votes.\n",
        "# Replace voting=\"hard\" with voting=\"soft\" and ensure that all your classifiers can estimate class probabilities.\n",
        "# If you use the SVC estimator, you need to set probability = True, which will make the SVC class use cross-validation to estimate class probabilities and add a predict_proba() method.\n",
        "#import classifiers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "# create models\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC(probability=True)\n",
        "\n",
        "# An ensemble of them\n",
        "# here we select hard voting, which returns the majority of the predictions, not an average of probabilities\n",
        "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svm', svm_clf)], voting='soft')\n",
        "\n",
        "# we can cycle throgh individual estimators\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "\n",
        "#fit the training set\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "#predict\n",
        "  y_pred_probabilties = clf.predict_proba(X_test)\n",
        "  print(y_pred_probabilties[0:5])\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
        "  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.54857026e-01 7.45142974e-01]\n",
            " [8.10150896e-01 1.89849104e-01]\n",
            " [1.17714770e-04 9.99882285e-01]\n",
            " [9.99833583e-01 1.66417496e-04]\n",
            " [7.12150048e-01 2.87849952e-01]]\n",
            "LogisticRegression 0.9263157894736842\n",
            "[[0.7 0.3]\n",
            " [0.9 0.1]\n",
            " [0.  1. ]\n",
            " [0.9 0.1]\n",
            " [0.4 0.6]]\n",
            "RandomForestClassifier 0.9263157894736842\n",
            "[[0.39458009 0.60541991]\n",
            " [0.39457771 0.60542229]\n",
            " [0.35463051 0.64536949]\n",
            " [0.3945775  0.6054225 ]\n",
            " [0.3945775  0.6054225 ]]\n",
            "SVC 0.6385964912280702\n",
            "[[0.38524109 0.61475891]\n",
            " [0.70367136 0.29632864]\n",
            " [0.11633887 0.88366113]\n",
            " [0.70023216 0.29976784]\n",
            " [0.50433765 0.49566235]]\n",
            "VotingClassifier 0.9333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJDKGlfsogkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e21af88c-c99e-40e7-930f-89a681434fd3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.svm import SVR\n",
        "reg1 = SVR(C=1.0, epsilon=0.2)\n",
        "reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n",
        "reg3 = LinearRegression()\n",
        "ereg = VotingRegressor(estimators=[('SV', reg1), ('rf', reg2), ('lr', reg3)])\n",
        "ereg = ereg.fit(boston_X_train, boston_y_train)\n",
        "y_pred = ereg.predict(boston_X_test)\n",
        "boston_score = ereg.score(boston_X_test, boston_y_test)\n",
        "print('Boston Score:\\n', boston_score)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boston Score:\n",
            " 0.6787662605280083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEitRvUIvzEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Excercise 8 - Building Pipelines\n",
        "# Build a pipeline so that many of the steps we have covered to now can be implemented in one step, and be applied to the training and testing set without repeating code.\n",
        "# import the libraries - these are dependent on what we wish to implement in our pipelines\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# this is the pipeline class required to implement all pipelines\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# here we instantiate the pipeline with our methods\n",
        "# takes an arbitary number of transfomers and ends with an estimator\n",
        "pipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(random_state=1))  \n",
        "# here we fit our pipeline to the data, so that the steps above are executed on the training set\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "# here we call the predict method on our pipeline model against the test set \n",
        "y_pred = pipe_lr.predict(X_test)\n",
        "# and print out the result\n",
        "print('Test Accuracy: %.3f' % accuracy_score(y_test, y_pred)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsU_3T-bw29G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "69fcf505-b421-4e0a-f913-e0eea12d4aa1"
      },
      "source": [
        "# Implement a pipeline on the classification dataset\n",
        "# Implement K-Fold Cross Validation - randomly split the training dataset into k-folds without replacement, where k-1 folds are used for the model training \n",
        "# and one fold is used for performance evaluation. This procedure is repeated k-times so that we obtain k models and performance estimates\n",
        "\n",
        "## K-Fold-CV is used for model tuning - finding the optimatal hyperameter values that yield a satisfying generalization performance\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# 5-fold cross-validation on our dataset\n",
        "kf = KFold(n_splits=5, random_state=1).split(X_train, y_train)\n",
        "# pipeline class required to implement all pipelines\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# here we instantiate the pipeline with our methods\n",
        "# scaling features to lie between a given minimum and maximum value,between zero and one\n",
        "pipe_KFCV = make_pipeline(MinMaxScaler(), PCA(n_components=5), LogisticRegression(random_state=1))  \n",
        "# here we fit our pipeline to the data, so that the steps above are executed on the training set\n",
        "pipe_KFCV.fit(X_train, y_train)\n",
        "# here we call the predict method on our pipeline model against the test set \n",
        "y_pred = pipe_KFCV.predict(X_test)\n",
        "#print the K-Fold-CV score\n",
        "scores = cross_val_score(estimator=pipe_KFCV, X=X_train, y=y_train, cv=kf, n_jobs=1)\n",
        "print('K Fold-CV score: %s' % scores)\n",
        "# and print out the result\n",
        "print('Test Accuracy: %.3f' % accuracy_score(y_test, y_pred)) "
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K Fold-CV score: [0.96491228 0.96491228 1.         0.96491228 0.96428571]\n",
            "Test Accuracy: 0.944\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}