{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pushpanjali_Banik_tf2_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/bRAd28FtQUi3ca0Z5rjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjali88/Data-Science/blob/master/Pushpanjali_Banik_tf2_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dEV0V9tAUgx",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will use another dataset - the **`mnist`** dataset -  to build on our knowledge. In particular, we will:\n",
        "  * introduce **`Computer Vision`** \n",
        "  * introduce **`convolutional layers`** into our models \n",
        "  * introduce the concept of **`regularisation`**\n",
        "  * introduce the **`validation set`** in training our model \n",
        "  * introduce how to **`save`** and reuse our model \n",
        "* The image below sets out how this fits within our deep learning framework and exising knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd6vVodbAl-n",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%202%20-%20Summary_final.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvwTjn1tAbRr",
        "colab_type": "code",
        "outputId": "26c487d2-1cf2-4aa5-d20e-c9329cb10610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "#Load Libraries\n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0LSGp87BwS4",
        "colab_type": "code",
        "outputId": "db2c1b9c-2ae0-455b-8044-f54b58f7f7bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#We are future proofing by importing modules that modify or replace exising modules that we may have used now \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#import helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print out the version we are using \n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS_8WCQFC8KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# additional imports for this notebook\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1miHVOwDGey",
        "colab_type": "text"
      },
      "source": [
        "# 1.2 Loading our Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omozhltdDLAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NHvV_R3DbxK",
        "colab_type": "code",
        "outputId": "f8e2fbba-886b-4efc-ffd4-72c88207b99a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# look at data\n",
        "plt.imshow(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fec2599fbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiTx/65XcTNOWsh5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDhH0_UKEnlQ",
        "colab_type": "code",
        "outputId": "79946dae-16e5-472a-f651-32e3a7be8923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#look at labels\n",
        "np.unique(train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg9toyDrEsl7",
        "colab_type": "text"
      },
      "source": [
        "**What is the problem we are trying to solve?**\n",
        "* As we can see, we have images of digits from 0 - 9, and labels from 0 - 9. We are trying to build a model that correctly classifies the digits in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVVXSv2UE8pN",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data: Introduction to Computer Vision \n",
        "\n",
        "**What is Computer Vision?**\n",
        "* Computer Vision is the field of how computers can gain understanding from images and videos. It includes tasks such as **`image recognition`** and **`object detection`**. Deep Learning is seen as the state of the art technology for solving computer vision problems. \n",
        "\n",
        "**Why is Deep Learning particularly good at it?**\n",
        "* The layers within a deep learning model are good for identifying and modelling the different aspects of an image (such as edges, parts of faces, and other important parts of an image). The meaning that each layer extracts can be built up to form representations for lots of different image types that can then be classified.  \n",
        "* In particular, **`convolutional layers`** are good at extracting representation from image data and they form the basis of deep learning models for image recognition. The ability to build larger and larger models that consist of these convolutional layers, and to train them with more and more data (thanks to increasing compute power), led to a leap forward in state of the art for computer vision. \n",
        "\n",
        "**How does it work?**\n",
        "* Every image is represented by an array of numbers. You may have noticed this when we looked at the **`shape`** of the images we were processing. This shape represents the number of pixels in an image, and each pixel has a numerical value. This numerical value maps to a colour value that is displayed. It is also what we use as input values to our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ewAAnAuE4mT",
        "colab_type": "code",
        "outputId": "53803741-33a4-4073-ca13-948c7f585d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## lets start by looking at the shape of an image\n",
        "\n",
        "## we can see that it is 28 x 28 pixels\n",
        "train_images[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8f4u7aAFN0E",
        "colab_type": "code",
        "outputId": "93431260-4841-4fc7-8996-245ed6354760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## we can also see that these pixels are represented in an array of numbers \n",
        "train_images[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pkrAPvTFSJ4",
        "colab_type": "code",
        "outputId": "076a67f5-4ef6-4a4e-e1ba-fb6e6ac096ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# we need plt.imshow() - or another library such as OpenCV or PIL - to output an image from this array\n",
        "plt.imshow(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fec259020f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiTx/65XcTNOWsh5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn_wP715F6m0",
        "colab_type": "text"
      },
      "source": [
        "**What do the array values mean?**\n",
        "* Each value leads to a colour for the pixel that the array value represents. Actualy what colour is displayed depends somewhat on the number of colour channels the array has. We have only one channel present in this dataset. This is grayscale channel. Typically, we will see three channels for colour images, with each channel representing one of Red, Green, Blue. A value in one channel will display a different colour than a value in another channel. \n",
        "* See the tutorials [here](https://www.w3schools.com/colors/default.asp) for more detail on how the values within a channel map to a colour.\n",
        "* Its worth noting here that there are typically 256 values (0 - 255) available in each channel, making a total combination of c. 16.8m colours available per a three channel image!\n",
        "* As per the previous notebook, we will rescale the arrays to between 0 and 1. This needs to happen in order to maximise the success of the training. \n",
        "\n",
        "**What about images of different shapes?**\n",
        "* The size of an image can and does vary. In this case, we have small image of 28 x 28 pixels (or 28, 28, 1) given we have one channel. This was the same for the previous dataset and it makes it easy to train models. \n",
        "* Outside of introductory tutorials, It is likely that you will see much larger images, meaning many more pixels and therefore larger arrays to train and learn representations on. This will make the models larger and training more involved. \n",
        "* One final thing to note is that Deep Learning models always require an array of the same size to be passed to it. This means that images which differ in size need to be preprocessed so that they are the same size before being passed to the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukmTUss2F7yh",
        "colab_type": "code",
        "outputId": "e8d5f6d7-5155-41ef-e331-ccee12469d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# we now need to reshape the data to add a colour channel \n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "# we can view the new shape \n",
        "print('\\n train image shape')\n",
        "print(train_images.shape)\n",
        "# normalize the data \n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " train image shape\n",
            "(60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AelRP-nHIbcA",
        "colab_type": "text"
      },
      "source": [
        "**What else can I learn to improve my knowledge?**\n",
        "* Images have to be fed into the model in the same shape each time. This requires pre-processing.\n",
        "* Prior to feed images into a model, we can also change the image in certain ways to add noise and variety to the training data. This should mean that the model is more robust and better at generalizing to unseen data. \n",
        "\n",
        "## 3. Model Building \n",
        "### 3.1 Convolutional Models\n",
        "\n",
        "**How do we build a convolutional neural network?**\n",
        "* A convolutional neural network (CNN) contains both dense and convolutional layers. The convolutional layers form the **`base`** of the model and extracts representation from the image. The dense layers form the **`head`** of the model and takes this represetation and maps it to our output classes\n",
        "* A convolutional layer takes our image as it (subject to any preprocessing to get it in a standard shape or augmented to add noise and variety to the dataset) - that is, we do not need to flatten the image into a **`1D`** array. We flatten the array after our final convolutional layer and prior to passing our input to the dense layer.\n",
        "\n",
        "**Why use a convolutional layer?**\n",
        "* A convolution better encodes the key information in an image than other types of layers. Their application to computer vision resulted in a marked improvement in what was state of the art. That's why we use them. \n",
        "\n",
        "**What is a convolutional layer?**\n",
        "* Simply, a convolutional layer is a layer that performs a mathematical operations known as convolutional on the input data.  In contrast, a dense layer perfornms matrix multiplicaiton on its inputs. \n",
        "* Each convolutional layer have a user-defined set of filters (or windows) that we pass over the image. We define the number and size of filters, although they are typically a 3 x 3 matrix. \n",
        "* This filter contains a set of weights that will be learned by the model and which are used to multiply the input values and return a new value in the layer's output. Its these filters that contain the learning of the convolutional layers of the model, whose weights will be updated as we train so that they are more and more able to extract key information from the image.\n",
        "* The filter is applied to all the image channels as it passes over each pixel location such that it will look at a specific row and column index position and all the array values available at that index:\n",
        "$$(row, column, :)$$\n",
        "* We won't go in to *how* convolutional works here, but see the cell at the end of this section for links that do explain how it works. \n",
        "\n",
        "**So what does a convolutional layer return?**\n",
        "* A convolutional layer returns an output array of the same (row, column) shape as the input array, but with one channel only. \n",
        "* It tends to be the case that convolutional layer is paired with a **`pooling layer`**. We won't cover these in any detail, but its sufficient to know that a pooling layer tries to extract the key information from the convolutional layer while typically halving its size. \n",
        "* The diagram below set this out. \n",
        "\n",
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%202%20Convolution%20and%20Pooling%20Layers.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euQS8p13Ie1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## lets build our convolutional base\n",
        "## we use the Sequential API but use .add() rather than passing the layers in as a list\n",
        "\n",
        "# build model using sequential \n",
        "model = models.Sequential() \n",
        "# start adding layers. input shape has been defined, including the channel value we added via reshape earlier\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# max pooling layers\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "# this is then repeated to build \n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# max pooling layers\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "# additional convolutional layer \n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUKXQr7JJ2Df",
        "colab_type": "code",
        "outputId": "7eefd77c-858a-4504-8784-9a9392ea169c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# print model \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNfUgqssKJta",
        "colab_type": "text"
      },
      "source": [
        "**How do we get to the parameter count?**\n",
        "* The parameters of a convolutional layer are defined by:\n",
        "$$((filter\\ height\\ \\times filter\\ width)\\ +\\ bias\\ term)\\ \\times number\\ of\\ filters$$\n",
        "\n",
        "* The bias term is a value of 1 so the number of parameters for the first convolutional layer is:\n",
        "$$((3 \\times 3)+ 1)\\ \\times 32 = 320$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**What about the classificaiton layer?**\n",
        "* As we said above, a convolutional based needs a classification layer to take the information extracted from an image and map it to output classes. \n",
        "* We take the final output shape of the Convolutional layer and flatten it to a 1D array. We then define our output layer, which in this instance is a layer of 10 with a softmax activation function. We have also added an additional layer to provide additional parameters between the flattened and output layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdeu9gZrKYQH",
        "colab_type": "code",
        "outputId": "5600602e-8191-4590-c2cc-5534711e1799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# flatten \n",
        "model.add(layers.Flatten())\n",
        "# add a fully connected layer\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "# add the output layer, a fully connected layer with a softmax activation\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxz0SEyCKtH2",
        "colab_type": "text"
      },
      "source": [
        "**What else can I learn to improve my knowledge?**\n",
        "* Understand more about convolution:\n",
        "  * There are a lot of good articles out there too. For example, [Cezanne Comacho](https://cezannec.github.io/Convolutional_Neural_Networks/), and [Chris Olah](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/) all provide a good understanding of how convolution works.\n",
        "  * For the maths of convolution, have a look at [this paper](https://arxiv.org/pdf/1603.07285.pdf).\n",
        "\n",
        "\n",
        "## 4. Training \n",
        "### 4.1 Validation Sets, Batch Sizes and Learning Rates\n",
        "\n",
        "**What will we cover now?**\n",
        "* We will cover more concepts around training the model: the purpose of a **`validation set;`** **`overfitting`**; and **`regularization`**. We will add in a validation and look for overfitting in our model performance. We won't add any regularization methods \n",
        "* We will also cover **`learning rates`** and  **`batch sizes`**. \n",
        "\n",
        "**What is validation set?**\n",
        "* A validation set is a dataset that is kept aside from the training dataset. Tpyically, at the end of each epoch, the trained model is passed the validation set in inference model, and the loss and other metrics recorded. The model is then trained again for another epoch, and at the end of this epoch is passed the validation set in inference mode. \n",
        "* This allows us to see during training how the model performs on unseen data and also whether the model is **`under- or over-fitting`**. \n",
        "\n",
        "**How do we create a validation set?**\n",
        "* We can set aside some data from our training set prior to beginning training, store it as a variable (or a set of (data, label) variables) and pass this to the validation_data argument in model.fit()\n",
        "* We can use the validation_split parameter in model.fit() to specify the fraction of the data to be used as training data.\n",
        "\n",
        "**What is under and overfitting?**\n",
        "* When we train our models, they can sometimes struggle to generalise well. This means that they do not perform well on unseen data. \n",
        "* A model that overfits will get better and better results (loss and metrics) on the training data and decreasing results on the validation set. This is because it is fitting too much to the specific characteristics of the training data, which may not be present in the unseen data.\n",
        "* A model that underfits does not perform well on either the training or validation data. \n",
        "* By incluidng a validation set,  we can monitor how well the model performs, and if the performance on the training and validation sets diverge too much then we can start to conclude that something need to be changed. \n",
        "\n",
        "**What can we do to address this?**\n",
        "* We can use **`regularization`** to help prevent overfitting.\n",
        "* We can regularize our model and its training in a number of ways, and to some extent they all penalize actions that may cause the model to overfit to the trianing data:\n",
        "  * When building a model, we add a **`dropout`** layer. This turns off a user defined number of outputs from a layer r during training and helps prevents the model becoming reliant on certain paths through the model.\n",
        "  *  During training, **`weight regularization`**. This penalises large weights (our learned parameters) in the model. Larger weights will create a larger loss value that the mdoel will then use to update the weights. A model with fewer larger weights, i.e. with the learning more evenly spread across the nodes, will have a better chance of generalizing well.   \n",
        "\n",
        "**What is a batch_size?**\n",
        "* A batch size is the number of samples the model trains on before performing a backward pass (model update). The number of batches in an epoch is equal to:\n",
        "$$ \\frac{number\\ of\\ sample\\ in\\ a\\ training\\ set}{batch\\ size}$$\n",
        "* We can set the batch size and choose to see the model performance as trains per batch size (using a parameter in **`model.fit()`**). \n",
        "\n",
        "**What is the learning rate?**\n",
        "* The learning rate controls the size of the update to the learnable parameters (weights and biases) during the backward pass, based on the loss of the model.\n",
        "* It is a parameter of the optimizer, set at the **`model.compile()`** stage, and can be set by the user. The trick is to set the learning to update the weights sufficiently to change performance, but not update them too much so that the model weights swing between higher and lower values without settling on a path to the best model.\n",
        "* There are various strategies for mitigating this problem that are worth investigating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VAnmnIsKuXD",
        "colab_type": "code",
        "outputId": "c6038fa9-87df-492e-cb44-48ef0d87ea16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "## Lets compile the model again\n",
        "model.compile(optimizer='adam',\n",
        "              \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## and build a training loop\n",
        "\n",
        "history = model.fit(train_images, train_labels, \n",
        "          # add batch size\n",
        "          batch_size = 32,\n",
        "          # epochs           \n",
        "          epochs = 10,\n",
        "          # and add a validation set \n",
        "          validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/10\n",
            "48000/48000 [==============================] - 53s 1ms/sample - loss: 0.1669 - accuracy: 0.9483 - val_loss: 0.0586 - val_accuracy: 0.9822\n",
            "Epoch 2/10\n",
            "48000/48000 [==============================] - 54s 1ms/sample - loss: 0.0481 - accuracy: 0.9846 - val_loss: 0.0529 - val_accuracy: 0.9841\n",
            "Epoch 3/10\n",
            "48000/48000 [==============================] - 52s 1ms/sample - loss: 0.0356 - accuracy: 0.9888 - val_loss: 0.0447 - val_accuracy: 0.9862\n",
            "Epoch 4/10\n",
            "48000/48000 [==============================] - 53s 1ms/sample - loss: 0.0265 - accuracy: 0.9915 - val_loss: 0.0311 - val_accuracy: 0.9908\n",
            "Epoch 5/10\n",
            "48000/48000 [==============================] - 53s 1ms/sample - loss: 0.0214 - accuracy: 0.9928 - val_loss: 0.0378 - val_accuracy: 0.9888\n",
            "Epoch 6/10\n",
            "48000/48000 [==============================] - 53s 1ms/sample - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.0415 - val_accuracy: 0.9876\n",
            "Epoch 7/10\n",
            "48000/48000 [==============================] - 53s 1ms/sample - loss: 0.0137 - accuracy: 0.9957 - val_loss: 0.0389 - val_accuracy: 0.9899\n",
            "Epoch 8/10\n",
            "48000/48000 [==============================] - 54s 1ms/sample - loss: 0.0115 - accuracy: 0.9964 - val_loss: 0.0375 - val_accuracy: 0.9899\n",
            "Epoch 9/10\n",
            "48000/48000 [==============================] - 52s 1ms/sample - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.0421 - val_accuracy: 0.9910\n",
            "Epoch 10/10\n",
            "48000/48000 [==============================] - 52s 1ms/sample - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.0360 - val_accuracy: 0.9908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_tN8N86SNZg",
        "colab_type": "text"
      },
      "source": [
        "Accuracy is 99% on validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWSnZF8TSS-X",
        "colab_type": "code",
        "outputId": "af0d86e0-1ac9-4a69-df01-c956aec3eb33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# plot loss and accuracy\n",
        "# if we print out our history object we can see that it now includes validation values\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waUHLWvIS6X0",
        "colab_type": "code",
        "outputId": "c87e08a7-d110-441b-a5b3-6ec5770188f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# pass to dataframe\n",
        "import pandas as pd\n",
        "\n",
        "# pass history to dataframe object\n",
        "history_df = pd.DataFrame(history_dict)\n",
        "\n",
        "print(history_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       loss  accuracy  val_loss  val_accuracy\n",
            "0  0.166867  0.948271  0.058602      0.982167\n",
            "1  0.048083  0.984563  0.052934      0.984083\n",
            "2  0.035623  0.988833  0.044679      0.986250\n",
            "3  0.026467  0.991458  0.031059      0.990833\n",
            "4  0.021376  0.992813  0.037821      0.988833\n",
            "5  0.015825  0.995021  0.041481      0.987583\n",
            "6  0.013664  0.995687  0.038927      0.989917\n",
            "7  0.011536  0.996396  0.037506      0.989917\n",
            "8  0.010196  0.996833  0.042076      0.991000\n",
            "9  0.009436  0.996708  0.035985      0.990750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmBoRHsOTcA3",
        "colab_type": "code",
        "outputId": "3abe8140-db6a-4c1c-d9a6-6d7dd43cccd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "# we can use plot functionality of pandas to quickly plot our results\n",
        "history_df.plot(figsize=(8,5))\n",
        "# tailor our plot. Show the grid\n",
        "plt.grid(True)\n",
        "# set the vertical range to [0 -1]\n",
        "plt.gca().set_ylim(0, 1)\n",
        "# display plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAEzCAYAAADkYKBTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+B0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j98UlrK8Tjh6",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Saving Models\n",
        "\n",
        "**What do we mean by *saving a model*?**\n",
        "* We can save our progress as we train our models. We can save our progress in two ways:\n",
        "  * *during training*, so that our models are saved after each epoch (or, after an epoch that shows model improvement). \n",
        "  * *after training*, so that our model has completed its training before we save it\n",
        "* Of course, we can opt to save the model both during *and* after training. \n",
        "* Using `tensorflow 2.0`, we can opt to save the model either manually, i.e. after the model has trained, or by using callbacks - i.e. incorporating saving into the training process. \n",
        "  \n",
        "**What are we saving?**\n",
        "* We can save the model weights only, the full model (including the weights and the architecture), and the optimizer state.\n",
        "* Its useful to remember that when we are training a model, the parameters we are updating during the training process are the weights at each layer of the model. Our aim is to train on (data, labels) pairs that mean we can predict effectively on unseen data using the weights we have trained. So it is these weights that are saved, optionally along with the model architecture. \n",
        "* Optimizer state. We haven't focussed too much on optimizers, but remember that this is the way that the model weights are updated. The size of the update is set by the (user defined) **`learning rate`**. When we save a model we can therefore save the **`optimizer-state`**, meaning we can continue training a loaded model from the state it was in when the model was saved.  \n",
        "\n",
        "**Why save a model?**\n",
        "* So that we can reuse it later. This could be to deploy it and use it in inference mode, or to continue training from the point at which we stopped. \n",
        "\n",
        "**Once we have saved a model, how do we use it again?**\n",
        "* Once we have saved our weights and/or model, we can restore the model in a couple of different ways. If we decide to save the weights only, we need to create an identical model to the one that was used to create our weights. If we saved both the model and weights, we can load this entire model.\n",
        "\n",
        "**What are the ways of doing it?**\n",
        "* In this tutorial (notebook 2), we will look at saving and loading model weights and model + weight manually, i.e. after training. \n",
        "* In notebook 3, we will look at how to save during and after training using callbacks. \n",
        "* We will use the Keras API. Note there are some other ways to save the model covered in the [tensorflow 2.0 tutorials](https://www.tensorflow.org/beta/tutorials/keras/save_and_restore_models) provided by Google.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skh_Q4e1TqTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## lets go through the steps of saving the model weights only\n",
        "\n",
        "# here we define a location for the weights to be saved \n",
        "model.save_weights('./checkpoints/my_checkpoint')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8iEdn6PUuRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to load the weights we need to create a new instance of the same model architecture \n",
        "new_instance = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00kcN73GUzRO",
        "colab_type": "code",
        "outputId": "b872e49d-7562-4586-bdcd-2cfc8d209606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# then we can load the weights \n",
        "new_instance.load_weights('./checkpoints/my_checkpoint')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fec259f1fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAZXFXDwU2nQ",
        "colab_type": "code",
        "outputId": "71e94c94-7e2d-4623-e995-27fc2e0de040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "new_instance.weights[0][0][0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
              "array([ 0.04556673, -0.26932698, -0.03634606, -0.31952682,  0.0472606 ,\n",
              "       -0.00215081,  0.08703726,  0.03582785, -0.29666814,  0.00171104,\n",
              "        0.08579903, -0.32776594, -0.2396187 ,  0.12862141, -0.00797031,\n",
              "        0.2182629 , -0.05623506, -0.28996643,  0.03816007, -0.03198093,\n",
              "        0.22306302,  0.26011452, -0.1934522 ,  0.05250368, -0.01064176,\n",
              "       -0.01398766, -0.06618564,  0.1420222 ,  0.07068293,  0.07796174,\n",
              "       -0.04445089,  0.26675996], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do9vQVKeU5XZ",
        "colab_type": "text"
      },
      "source": [
        "#### 4.3.2 Saving and Loading an entire model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXn0_RkxU7rv",
        "colab_type": "code",
        "outputId": "fb88cea4-7833-409c-922e-2023f6685b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# save model to HDF5 format\n",
        "model.save('my_model.h5')\n",
        "\n",
        "# recreate the saved model, including weights and optimizer \n",
        "new_model = keras.models.load_model('my_model.h5')\n",
        "\n",
        "# view summary\n",
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoMj8FBNWDEq",
        "colab_type": "text"
      },
      "source": [
        "###So, what did we cover in this section?\n",
        "* Validation sets\n",
        "* Overfitting \n",
        "* Regularization\n",
        "* Learning Rates, Batch Sizes\n",
        "* Saving Models\n",
        "\n",
        "**How does it add to our existing knowledge?**\n",
        "* We have built on our undertanding of the training loop by adding different aspects to it such as batch size and validation set, as well as getting a feel for what we need to guard against in training models (e.g. overfitting)\n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* In the next notebook, we will use a callback to save a model as it trains. \n",
        "* Overfitting - there is much more to this topic. There is a good treatment of this subject in [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/). \n",
        "\n",
        "##5. Evaluation and Inference "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ytH_42kWErE",
        "colab_type": "code",
        "outputId": "0c3f8f82-b6ac-4132-9d9c-ce48044a6d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## using the model we have just saved, evaluate it on the test set \n",
        "## if you are stuck, use some of the code from notebook 1\n",
        "# \n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "## if we had saved our weights or model and loaded them up, we would use the model\n",
        "## variable name we had saved here"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 379us/sample - loss: 0.0325 - accuracy: 0.9912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJEpaA29NnI3",
        "colab_type": "code",
        "outputId": "bcd9cb9a-6a06-4812-d9e8-4082d8b5dd21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# show how the model performs in inference mode. Again, use some code from previous notebook\n",
        "print(test_acc)\n",
        "print(test_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9912\n",
            "0.032465098444359686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msd20ulfx5K7",
        "colab_type": "text"
      },
      "source": [
        "## 6. Summary\n",
        "\n",
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%202%20-%20Summary_final.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyNa94NH5ytp",
        "colab_type": "text"
      },
      "source": [
        "# 8. Excercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OsTsB0L5tnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## build another convolution model\n",
        "## add more layers, vary their size and look up dropout and how to add that\n",
        "\n",
        "\n",
        "## build our convolutional base\n",
        "## use the Sequential API but use .add() rather than passing the layers in as a list\n",
        "\n",
        "# build model using sequential \n",
        "model1 = models.Sequential() \n",
        "# start adding layers. input shape has been defined, including the channel value we added via reshape earlier\n",
        "model1.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model1.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# max pooling layers\n",
        "model1.add(layers.MaxPooling2D((2,2)))\n",
        "# this is then repeated to build \n",
        "model1.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model1.add(layers.MaxPooling2D((2,2)))\n",
        "model1.add(layers.Dropout(0.25))\n",
        "model1.add(layers.Flatten())\n",
        "# additional layer \n",
        "model1.add(layers.Dense(128, activation='relu'))\n",
        "model1.add(layers.Dropout(0.5))\n",
        "model1.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu48SMP5_Z1L",
        "colab_type": "code",
        "outputId": "5f0da255-55bb-40e9-8865-7b140673c015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 10, 10, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 261,962\n",
            "Trainable params: 261,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VdegDEh_dfM",
        "colab_type": "code",
        "outputId": "da0fee63-17b8-47f8-d1ef-c12b35ec6677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "#compile the model\n",
        "model1.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#fit the model\n",
        "history1 = model1.fit(train_images, train_labels,\n",
        "          batch_size= 32,\n",
        "          epochs=10,\n",
        "          validation_split=0.3)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "## evaluate the model and use it inference mode \n",
        "test_loss1, test_acc1 = model1.evaluate(test_images, test_labels)\n",
        "print(test_acc1)\n",
        "print(test_loss1)\n",
        "\n",
        "## save the model \n",
        "model1.save('my_model1.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 18000 samples\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 143s 3ms/sample - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.0354 - val_accuracy: 0.9905\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 143s 3ms/sample - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.0344 - val_accuracy: 0.9915\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 143s 3ms/sample - loss: 0.0238 - accuracy: 0.9925 - val_loss: 0.0288 - val_accuracy: 0.9932\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 142s 3ms/sample - loss: 0.0202 - accuracy: 0.9935 - val_loss: 0.0299 - val_accuracy: 0.9929\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 142s 3ms/sample - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.0382 - val_accuracy: 0.9916\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 141s 3ms/sample - loss: 0.0200 - accuracy: 0.9938 - val_loss: 0.0368 - val_accuracy: 0.9914\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 141s 3ms/sample - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.0317 - val_accuracy: 0.9928\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 141s 3ms/sample - loss: 0.0160 - accuracy: 0.9942 - val_loss: 0.0412 - val_accuracy: 0.9913\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 141s 3ms/sample - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0370 - val_accuracy: 0.9917\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 141s 3ms/sample - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0382 - val_accuracy: 0.9923\n",
            "\n",
            "\n",
            "10000/10000 [==============================] - 9s 876us/sample - loss: 0.0228 - accuracy: 0.9943\n",
            "0.9943\n",
            "0.02278540663353216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1BfndcADMdV",
        "colab_type": "code",
        "outputId": "17a19460-2472-4fc9-89a5-4f72bd8028ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "source": [
        "# plot loss and accuracy\n",
        "# if we print out our history object we can see that it now includes validation values\n",
        "history_dict1 = history1.history\n",
        "print(history_dict1.keys())\n",
        "\n",
        "print(\"\\n\")\n",
        "history_df1 = pd.DataFrame(history_dict1)\n",
        "print(history_df1)\n",
        "\n",
        "# we can use plot functionality of pandas to quickly plot our results\n",
        "history_df1.plot(figsize=(10,6))\n",
        "# tailor our plot. Show the grid\n",
        "plt.grid(True)\n",
        "# set the vertical range \n",
        "plt.gca().set_ylim(0, 1)\n",
        "# display plot\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "\n",
            "\n",
            "       loss  accuracy  val_loss  val_accuracy\n",
            "0  0.024676  0.991976  0.035439      0.990500\n",
            "1  0.023860  0.992310  0.034404      0.991500\n",
            "2  0.023785  0.992500  0.028803      0.993222\n",
            "3  0.020168  0.993548  0.029872      0.992944\n",
            "4  0.019315  0.993905  0.038243      0.991556\n",
            "5  0.019964  0.993810  0.036769      0.991389\n",
            "6  0.017682  0.994405  0.031680      0.992833\n",
            "7  0.015954  0.994190  0.041183      0.991333\n",
            "8  0.018124  0.993881  0.037014      0.991667\n",
            "9  0.015186  0.995405  0.038197      0.992278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAFpCAYAAABeYWb6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAJ4QrAAAADwhWAEAAHhCsAIAAPDk/wMfx9OCdDHf3gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTLYrAq0APFh",
        "colab_type": "text"
      },
      "source": [
        "# 9. Further Reading\n",
        "* Work through Chapter 15 - page 517 - of Python Machine Learning for another implementation of a Convolutional Neural Network\n",
        "* Optionally, work through Chapter 14 for a deeper understanding of tensorflow 2.0"
      ]
    }
  ]
}